{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid World Environment\n",
    "class GridWorld:\n",
    "    def __init__(self, size=4):\n",
    "        self.size = size\n",
    "        self.goal = (size - 1, size - 1)  # Bottom-right corner is the goal\n",
    "        self.actions = ['up', 'down', 'left', 'right']  # Possible actions\n",
    "        self.state = (0, 0)  # Start at top-left corner\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset the agent to the starting position.\"\"\"\n",
    "        self.state = (0, 0)\n",
    "        return self.state\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Take a step in the environment based on the action.\"\"\"\n",
    "        x, y = self.state\n",
    "\n",
    "        # Perform the action\n",
    "        if action == 'up':\n",
    "            x = max(x - 1, 0)\n",
    "        elif action == 'down':\n",
    "            x = min(x + 1, self.size - 1)\n",
    "        elif action == 'left':\n",
    "            y = max(y - 1, 0)\n",
    "        elif action == 'right':\n",
    "            y = min(y + 1, self.size - 1)\n",
    "\n",
    "        self.state = (x, y)\n",
    "\n",
    "        # Check if the goal is reached\n",
    "        if self.state == self.goal:\n",
    "            reward = 10  # Reward for reaching the goal\n",
    "            done = True  # Episode ends\n",
    "        else:\n",
    "            reward = -1  # Small penalty for each step\n",
    "            done = False\n",
    "\n",
    "        return self.state, reward, done\n",
    "    \n",
    "    # Render the Grid World\n",
    "    def render(self):\n",
    "        \"\"\"\n",
    "        Visualize the grid.\n",
    "        \"\"\"\n",
    "        grid = np.full((self.size, self.size), \".\", dtype=str)  # Initialize grid with \".\"\n",
    "        grid[self.goal] = \"G\"  # Mark the goal\n",
    "        grid[self.state] = \"A\"  # Mark the agent's position\n",
    "\n",
    "        # Print the grid\n",
    "        print(\"\\n\".join(\" \".join(row) for row in grid))\n",
    "        print()\n",
    "\n",
    "# Q-Learning Algorithm\n",
    "class QLearning:\n",
    "    def __init__(self, env, alpha=0.1, gamma=0.9, epsilon=0.1):\n",
    "        self.env = env\n",
    "        self.alpha = alpha  # Learning rate\n",
    "        self.gamma = gamma  # Discount factor\n",
    "        self.epsilon = epsilon  # Exploration rate\n",
    "        self.q_table = np.zeros((env.size, env.size, len(env.actions)))  # Q-table\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        \"\"\"Choose an action using epsilon-greedy strategy.\"\"\"\n",
    "        if np.random.uniform(0, 1) < self.epsilon:\n",
    "            return np.random.choice(self.env.actions)  # Explore: random action\n",
    "        else:\n",
    "            x, y = state\n",
    "            return self.env.actions[np.argmax(self.q_table[x, y])]  # Exploit: best action\n",
    "\n",
    "    def update_q_table(self, state, action, reward, next_state):\n",
    "        \"\"\"Update the Q-table using the Q-learning formula.\"\"\"\n",
    "        x, y = state\n",
    "        next_x, next_y = next_state\n",
    "        action_idx = self.env.actions.index(action)\n",
    "\n",
    "        # Q-learning formula\n",
    "        old_value = self.q_table[x, y, action_idx]\n",
    "        next_max = np.max(self.q_table[next_x, next_y])\n",
    "        new_value = old_value + self.alpha * (reward + self.gamma * next_max - old_value)\n",
    "\n",
    "        self.q_table[x, y, action_idx] = new_value\n",
    "\n",
    "\n",
    "\n",
    "# Training the Agent\n",
    "def train_agent(env, agent, episodes=1000, render_every=100):\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            action = agent.choose_action(state)  # Choose action\n",
    "            next_state, reward, done = env.step(action)  # Take action\n",
    "            agent.update_q_table(state, action, reward, next_state)  # Update Q-table\n",
    "            state = next_state\n",
    "\n",
    "        if (episode + 1) % render_every == 0:\n",
    "            print(f\"Episode {episode + 1} completed\")\n",
    "            env.render()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the Agent\n",
    "def test_agent(env, agent):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    steps = 0\n",
    "\n",
    "    print(\"Testing the agent...\")\n",
    "    while not done:\n",
    "        action = agent.choose_action(state)  # Choose action (no exploration)\n",
    "        next_state, reward, done = env.step(action)\n",
    "        print(f\"State: {state}, Action: {action}, Next State: {next_state}, Reward: {reward}\")\n",
    "        state = next_state\n",
    "        steps += 1\n",
    "\n",
    "    print(f\"Goal reached in {steps} steps!\")\n",
    "    env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the agent...\n",
      "Episode 100 completed\n",
      ". . . .\n",
      ". . . .\n",
      ". . . .\n",
      ". . . A\n",
      "\n",
      "Episode 200 completed\n",
      ". . . .\n",
      ". . . .\n",
      ". . . .\n",
      ". . . A\n",
      "\n",
      "Episode 300 completed\n",
      ". . . .\n",
      ". . . .\n",
      ". . . .\n",
      ". . . A\n",
      "\n",
      "Episode 400 completed\n",
      ". . . .\n",
      ". . . .\n",
      ". . . .\n",
      ". . . A\n",
      "\n",
      "Episode 500 completed\n",
      ". . . .\n",
      ". . . .\n",
      ". . . .\n",
      ". . . A\n",
      "\n",
      "Episode 600 completed\n",
      ". . . .\n",
      ". . . .\n",
      ". . . .\n",
      ". . . A\n",
      "\n",
      "Episode 700 completed\n",
      ". . . .\n",
      ". . . .\n",
      ". . . .\n",
      ". . . A\n",
      "\n",
      "Episode 800 completed\n",
      ". . . .\n",
      ". . . .\n",
      ". . . .\n",
      ". . . A\n",
      "\n",
      "Episode 900 completed\n",
      ". . . .\n",
      ". . . .\n",
      ". . . .\n",
      ". . . A\n",
      "\n",
      "Episode 1000 completed\n",
      ". . . .\n",
      ". . . .\n",
      ". . . .\n",
      ". . . A\n",
      "\n",
      "\n",
      "Testing the agent...\n",
      "Testing the agent...\n",
      "State: (0, 0), Action: right, Next State: (0, 1), Reward: -1\n",
      "State: (0, 1), Action: right, Next State: (0, 2), Reward: -1\n",
      "State: (0, 2), Action: down, Next State: (1, 2), Reward: -1\n",
      "State: (1, 2), Action: down, Next State: (2, 2), Reward: -1\n",
      "State: (2, 2), Action: down, Next State: (3, 2), Reward: -1\n",
      "State: (3, 2), Action: right, Next State: (3, 3), Reward: 10\n",
      "Goal reached in 6 steps!\n",
      ". . . .\n",
      ". . . .\n",
      ". . . .\n",
      ". . . A\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env = GridWorld(size=4)\n",
    "agent = QLearning(env)\n",
    "\n",
    "print(\"Training the agent...\")\n",
    "train_agent(env, agent, episodes=1000, render_every=100)\n",
    "\n",
    "print(\"\\nTesting the agent...\")\n",
    "test_agent(env, agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Grid World representation for a neural network](https://stackoverflow.com/questions/36850302/grid-world-representation-for-a-neural-network)\n",
    "- [Fundamentals of Reinforcement Learning: Navigating Gridworld with Dynamic Programming](https://medium.com/gradientcrescent/fundamentals-of-reinforcement-learning-navigating-gridworld-with-dynamic-programming-9b98a6f20310)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
