{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation\n",
    "\n",
    "> A feedforward neural network with two hidden layers and one output unit, designed for regression tasks using Mean Squared Error **(MSE)** loss and ReLU activations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- ReLU activation\n",
    "- 2 hidden layers, each with 2 neurons\n",
    "- 3 input features\n",
    "- 1 output unit\n",
    "- Mean Squared Error (MSE) loss\n",
    "- Backpropagation implemented manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize parameters\n",
    "np.random.seed(0)\n",
    "input_size = 3\n",
    "hidden1_size = 2\n",
    "hidden2_size = 2\n",
    "output_size = 1\n",
    "lr = 0.01  # learning rate\n",
    "\n",
    "# Weights and biases\n",
    "W1 = np.random.randn(input_size, hidden1_size)\n",
    "b1 = np.zeros((1, hidden1_size))\n",
    "\n",
    "W2 = np.random.randn(hidden1_size, hidden2_size)\n",
    "b2 = np.zeros((1, hidden2_size))\n",
    "\n",
    "W3 = np.random.randn(hidden2_size, output_size)\n",
    "b3 = np.zeros((1, output_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReLU and its derivative\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return (x > 0).astype(float)\n",
    "\n",
    "# Mean Squared Error and its derivative\n",
    "def mse(y_true, y_pred):\n",
    "    return np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "def mse_derivative(y_true, y_pred):\n",
    "    return 2 * (y_pred - y_true) / y_true.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "def train(X, y, epochs=1000):\n",
    "    global W1, b1, W2, b2, W3, b3\n",
    "    for epoch in range(epochs):\n",
    "        # Forward pass\n",
    "        z1 = X @ W1 + b1\n",
    "        a1 = relu(z1)\n",
    "\n",
    "        z2 = a1 @ W2 + b2\n",
    "        a2 = relu(z2)\n",
    "\n",
    "        z3 = a2 @ W3 + b3\n",
    "        y_pred = z3  # linear output layer\n",
    "\n",
    "        # Compute loss\n",
    "        loss = mse(y, y_pred)\n",
    "\n",
    "        # Backward pass\n",
    "        dL_dy = mse_derivative(y, y_pred)\n",
    "\n",
    "        # Layer 3\n",
    "        dL_dW3 = a2.T @ dL_dy\n",
    "        dL_db3 = np.sum(dL_dy, axis=0, keepdims=True)\n",
    "\n",
    "        # Layer 2\n",
    "        dL_da2 = dL_dy @ W3.T\n",
    "        dL_dz2 = dL_da2 * relu_derivative(z2)\n",
    "        dL_dW2 = a1.T @ dL_dz2\n",
    "        dL_db2 = np.sum(dL_dz2, axis=0, keepdims=True)\n",
    "\n",
    "        # Layer 1\n",
    "        dL_da1 = dL_dz2 @ W2.T\n",
    "        dL_dz1 = dL_da1 * relu_derivative(z1)\n",
    "        dL_dW1 = X.T @ dL_dz1\n",
    "        dL_db1 = np.sum(dL_dz1, axis=0, keepdims=True)\n",
    "\n",
    "        # Update weights and biases\n",
    "        W3 -= lr * dL_dW3\n",
    "        b3 -= lr * dL_db3\n",
    "        W2 -= lr * dL_dW2\n",
    "        b2 -= lr * dL_db2\n",
    "        W1 -= lr * dL_dW1\n",
    "        b1 -= lr * dL_db1\n",
    "\n",
    "        # Print loss occasionally\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.0672\n",
      "Epoch 100, Loss: 0.0543\n",
      "Epoch 200, Loss: 0.0537\n",
      "Epoch 300, Loss: 0.0536\n",
      "Epoch 400, Loss: 0.0535\n",
      "Epoch 500, Loss: 0.0535\n",
      "Epoch 600, Loss: 0.0534\n",
      "Epoch 700, Loss: 0.0534\n",
      "Epoch 800, Loss: 0.0534\n",
      "Epoch 900, Loss: 0.0534\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "X = np.random.rand(10, 3)  # 10 samples, 3 features\n",
    "y = np.random.rand(10, 1)  # 10 target values\n",
    "\n",
    "train(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Gradient Descent](https://en.wikipedia.org/wiki/Gradient_descent)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
