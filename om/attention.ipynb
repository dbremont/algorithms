{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention\n",
    "\n",
    "> Attention is a mechanism in neural networks that allows the model to focus on specific parts of the input sequence by assigning different weights to different elements, enabling it to capture dependencies and relationships more effectively.\n",
    "\n",
    "> Note there are many others attention mechanisms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each row.\"\"\"\n",
    "    exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))  # Stability trick\n",
    "    return exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n",
    "\n",
    "def scaled_dot_product_attention(Q, K, V):\n",
    "    \"\"\"\n",
    "    Compute scaled dot-product attention.\n",
    "    \n",
    "    Args:\n",
    "    - Q: Query matrix (seq_len, d)\n",
    "    - K: Key matrix (seq_len, d)\n",
    "    - V: Value matrix (seq_len, d)\n",
    "    \n",
    "    Returns:\n",
    "    - Attention output matrix (seq_len, d)\n",
    "    \"\"\"\n",
    "    d_k = Q.shape[-1]  # Dimension of keys (same as queries)\n",
    "    \n",
    "    # Step 1: Compute attention scores (QK^T)\n",
    "    scores = np.dot(Q, K.T) / np.sqrt(d_k)\n",
    "    \n",
    "    # Step 2: Apply softmax to get attention weights\n",
    "    attention_weights = softmax(scores)\n",
    "    \n",
    "    # Step 3: Multiply attention weights by values (weighted sum)\n",
    "    output = np.dot(attention_weights, V)\n",
    "    \n",
    "    return output, attention_weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input embeddings (X):\n",
      " [[0.37454012 0.95071431 0.73199394 0.59865848]\n",
      " [0.15601864 0.15599452 0.05808361 0.86617615]\n",
      " [0.60111501 0.70807258 0.02058449 0.96990985]]\n",
      "\n",
      "Attention Weights:\n",
      " [[0.52735889 0.11510842 0.35753269]\n",
      " [0.43524369 0.19971446 0.36504185]\n",
      " [0.5060941  0.12923692 0.36466898]]\n",
      "\n",
      "Attention Output:\n",
      " [[0.9770141  0.93531103 1.15681926 1.43938704]\n",
      " [0.87857753 0.86060494 1.04849229 1.32790919]\n",
      " [0.9564023  0.92013827 1.13535784 1.41721063]]\n"
     ]
    }
   ],
   "source": [
    "# Define a small sequence of 3 input token embeddings (each of size 4)\n",
    "np.random.seed(42)  # For reproducibility\n",
    "X = np.random.rand(3, 4)  # (seq_len=3, d=4)\n",
    "\n",
    "# Simulate learned weight matrices (random for now)\n",
    "W_Q = np.random.rand(4, 4)  # Projection matrix for Q\n",
    "W_K = np.random.rand(4, 4)  # Projection matrix for K\n",
    "W_V = np.random.rand(4, 4)  # Projection matrix for V\n",
    "\n",
    "# Compute Q, K, V\n",
    "Q = X @ W_Q\n",
    "K = X @ W_K\n",
    "V = X @ W_V\n",
    "\n",
    "# Compute attention output\n",
    "output, attention_weights = scaled_dot_product_attention(Q, K, V)\n",
    "\n",
    "# Print results\n",
    "print(\"Input embeddings (X):\\n\", X)\n",
    "print(\"\\nAttention Weights:\\n\", attention_weights)\n",
    "print(\"\\nAttention Output:\\n\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Attention](https://en.wikipedia.org/wiki/Attention_(machine_learning))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
